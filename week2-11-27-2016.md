# Mafia Week 2
11/27/2016, Presented by **Zhenbang Liu** and ** Hongqiao Li**.

This week, Liu talked about concurrency control of distribution system. Li focused on sharing knowledge with different Knapsack problems.
# Content

**[Distributed System](#DS)**
* [Schedule](#schedule)
* [Two-phase Locking](#2pl)
* [Strict Two-phase Locking](#s2pl)
* [Lock Manager](#lm)
* [Deadlock](#dl)

**[Knapsack Problems](#kp)**
* [0/1 Knapsack](#01kp)
* [Unbounded Knapsack](#ubkp)
* [Bounded Knapsack](#bkp)
* [Group Knapsack](#gkp)
* [Q&A](#qa)

**[Reference](#ref)**

## Distribution System{#DS}
Today's focus is on **Isolation:**

**Serial schedule** is the most safe way to ensure isolation, because the system executes transactions one by one. However, this can be inefficient, especially when the previous transaction involves I/O. So, we have to find a another equivalent schedule that produces the same result as the serial one, but allows multiple transactions execute concurrently. (Similar as the schedule of multi-threads in OS)

### Schedule{#schedule}

Describe the execution of transactions running in the system.

#### Serial

Schedule that only executes one transaction at one time. For example:

![](https://wikimedia.org/api/rest_v1/media/math/render/svg/40439c64556cdb6d8e9e4477a10baed3602bdc84)
>> Schedule D is a serial schedule.


#### Serializable

Serial schedule is safe but slow, so we try to find sechdules **equivalent** to serial. For example:

![](https://wikimedia.org/api/rest_v1/media/math/render/svg/231904e2f4fe317bc62adbe0dd934eb27218bd7a)
>> Schedule E is equivalent to previous transaction D.


Check Confilct Actions to find a serializable schdule. Two actions **conflict** if:
* They are by different transactions. 
* They are on the same objects. 
* At least one of them is a write. 

For example:
```
R1(X), W2(X). 
These two actions conflict, because the execution order of these two actions cannot be changed.
```

Two schedules are **conflict equivalent** if:
* Same transactions are involved. 
* Every pair of conflicting actions is ordered the same way \(i.e. the execution order of these two actions **cannot** be changed\).

And, if one schedule has some conflict equivalent schdules, it is **conflict serializable**.

#### Ways to determine conflict serializable

* If the schedule can be transform into a serial schedule by swapping **consecutive non-conflicting** actions of different transactions.

   For example:

   ```
   T1: R(A)W(A)         R(B)W(B)
   T2:         R(A)W(A)         R(B)W(B)

   T1: R(A)W(A)       R(B)     W(B)               //R(B) and W(A) are consecutive
   T2:             R(A)    W(A)    R(B)W(B)       //they are on differenct object, so they are non-conflicting

   T1: R(A)W(A)R(B)         W(B)
   T2:             R(A)W(A)         R(B)W(B)

   T1: R(A)W(A)R(B)    W(B)     
   T2:             R(A)    W(A)R(B)W(B)

   T1: R(A)W(A)R(B)W(B)                          //Transformed into a serial schdule.
   T2:                 R(A)W(A)R(B)W(B)
   ```

* Dependency graph
 
    In a dependency graph, every node represents a transaction. An edge from T1 to T2 if an operation O1 of T1 conflicts with an operation O2 of T2 and O1 appears earlier in the schedule than O2. 
    
    If the dependency graph is acyclic, the schedule is **conflict serializable**. 
    
    For example:

   ```
   T1:R(A)W(A)                R(B)W(B)    //W(A) and R(A) are conflict, and W(A) appears before R(A)
   T2:        R(A)W(A)R(B)W(B)            //W(B) and R(B) are conflict, and W(B) appears before R(B)
   The dependency graph is like the following. Cycle exsists so this schdule is not conflict serializable.
   ```

   ![](/assets/DG2.png)


### Two-Phase Locking \(2PL\){#2pl}

Two-phase locking protocol is one in which there are 2 phases that a transaction goes through:

* The first is the growing phase in which it is acquiring locks, 
* the second is one in which it is releasing locks. Once you have released a lock, you cannot acquire any more locks.

This protocol ensures a serializable schedule. Because that once a transaction has locked an object it does not unlock it until it has locked all the objects required by all its operations. This ensures that two transactions must access any objects they have in common in the same order.

For example:

If we have a schedule like the following table, how can we ensure the serializablity

| T1 | T2 |
| --- | --- |
| R\(A\) |  |
| W\(A\) |  |
|  | R\(A\) |
|  | W\(A\) |
|  | R\(B\) |
|  | W\(B\) |
| R\(B\) |  |
| W\(B\) |  |
| Commit |  |
|  | Commit |

Using Two-phase lock:

| T1 | T2 |
| --- | --- |
| XL\(A\) |  |
| R\(A\) |  |
| W\(A\) |  |
|  | !XL\(A\) |
| XL\(B\) |  |
| R\(B\) |  |
| W\(B\) |  |
| UL\(A\) |  |
| UL\(B\) |  |
|  | XL\(A\) |
|  | R\(A\) |
|  | W\(A\) |
|  | XL\(B\) |
|  | R\(B\) |
|  | W\(B\) |
|  | UL\(A\) |
|  | UL\(B\) |
| Commit |  |
|  | Commit |
Now the this is a serialized schedule.

### Strict Two-phase Locking{#s2pl}

However, two-phase locking does not guard against cascading aborts for the following reason:

Suppose a transaction locks all its objects, in two-phase lock, it  will unlock them after each object is used. Then that object can be seen by other transactions **before this one completes**. If this one aborts, we must abort all such transactions.

In the previous example, what if T1 aborts instead of commits? 
Since there is a dependency of T2 on T1, if T1 aborts, T2 aborts as well.

In order to avoid such situation, strict two-phase locking moves the unlock phase after one transaction has completed.

Previous example in Strict two-phase locking:

| T1 | T2 |
| --- | --- |
| XL\(A\) |  |
| R\(A\) |  |
| W\(A\) |  |
|  | !XL\(A\) |
| XL\(B\) |  |
| R\(B\) |  |
| W\(B\) |  |
| Commit |  |
| UL\(A\) |  |
| UL\(B\) |  |
|  | XL\(A\) |
|  | R\(A\) |
|  | W\(A\) |
|  | XL\(B\) |
|  | R\(B\) |
|  | W\(B\) |
|  | Commit |
|  | UL\(A\) |
|  | UL\(B\) |
Looking at the schedule – we have ended up with a serial schedule and you might ask why go to all that work just to end with a serial schedule. Remember we are using conflicting operations to illustrate the point. If the operations were not conflicting, there would be no issues interleaving operations.

Some questions worth knowing about two-phase locking and concurrency Control: [Chapter 19 Concurrency Control](https://www.cl.cam.ac.uk/teaching/2000/ConcSys/csig2/57.html)

### Lock Management{#lm}

Lock and unlock requests handled by Lock Manager.

Lock Manager keeps an entry for each currently held lock.
Entry contains:

* List of transactions currently holding lock
* Type of lock held
* Queue of lock requests

The diagram of lock manager may seem like this:

![](/assets/LM.png)

When lock request arrives,check if this lock is held by any other transactions:
  * If no, grant the lock. 
  * If yes, put the requestor into the wait queue.

Lock upgrade:
  * Transactions with shared lock can request to upgrade to exclusive.

###Deadlocks{#dl}
Cycle of transactions waiting for locks to be released by each other. 
Two ways of dealing with deadlocks:
* Prevention
* Detection

**Detection**:

Wait-For Graph:
  * Each node in wait-for graph represents a transaction.
  * If T1 is waiting for locks to be released by T2, put edge from T1 to T2.

If cycle exists in the graph, deadlock exists.
 
 For example:
 ```
   T1:S(A)S(D)    S(B)
   T2:        X(B)            X(C)
   T3:                S(D)S(C)        X(A)
   T4:                            X(B)
 ```
 ![](/assets/wait-for.png)
 
 **Prevention**:
 
 Assign priorities based on timestamps.
 
 For example:
 If Ti wants a lock that Tj holds,
  * Wait-Die: If Ti has higher prioirty, Ti waits for Tj; otherwise Ti aborts
  * Wound-wait: If Ti has higher prioirty, Tj aborts; otherwise Ti waits.

**Important detail : If a transaction re-starts, make sure it gets its original timestamp.** Otherwise it will always be put in the end of execution queue.

## Knapsack Problems{#kp}
There are kinds of Knapsack problems but the following four is typical.
### 0/1 Knapsack{#01kp}
Given two integer arrays val[0..n-1] and wt[0..n-1] which represent values and weights of n items. Also, given an integer W which represents bag capacity, find out the maximum value subset of val[] such that sum of the weights of this subset is smaller than or equal to W. You can either pick the complete item, or don’t pick it.

A simple brute force solution is to calculate all the subsets of the values such that the sum of their weight is less than W, then find the maximum value subset. The subsets of an array with n elements is 2^n. When n becomes large, it's not a better way to solve this problem. Any optimal solution? The answer is definite. We can use the dynamic problem to solve this problem.
#### Optimal solution-Dynamic Solution
For each items in the array, there are two cases: 1) included in the optimal subset,2) not included in the optimal subset. Thus, we can get the optimal solutions for their subproblems, so the definition of `dp[i][j]` can be as follows:

1) Maximum value obtained by `(i-1)` items and `j` weight, the `ith` item not included.

2) Maximum value obtained by `(i-1)` items and `(j-weight of the ith item)`, the `ith` item included.

The following shares two ways of the dynamic implementation.
#### Top-down recursion
```python
def knapSack_top(wts,vals,n,maxw):
    if n==0 or maxw==0:
        return 0
    # If weight of the nth item is more than capacity.
    # it can't be included
    if (wts[n-1]>maxw):
        return knapSack_top(wts,vals,n-1,maxw)
    else:
        return max(val[n-1]+knapSack_top(wts,vals,n-1,maxw-wts[n-1]),
                   knapSack_top(wts,vals,n-1,maxw))
```
#### Bottom-up memorization
```python
def knapSack_bottom(wts,vals,maxw):
    n=len(vals)
    if n==0 or maxw==0:
        return 0
    dp=[[0 for _ in xrange(maxw+1)] for _ in xrange(n+1)]

    for i in xrange(1,n+1):
        for j in xrange(maxw+1):
            if wts[i-1]<=j:
                dp[i][j]=max(dp[i-1][j],dp[i-1][j-wts[i-1]]+vals[i-1])
            else:
                dp[i][j]=dp[i-1][j]
    return dp[n][maxw]
```
Total time complexity is `O(n*maxW)`, we can do the space optimal work when you understand how it works.

### Unbounded Knapsack{#ubkp}
Unbounded means that there are infinite item available for each item.In other words, given a bag with capacity with W, picking as many items into the knapsack such that the sum of the items is maximized. We can define the problem as follows:
```
n1 means the number of type 1 packed into the knapsack
n2 means the number of type 2 packed into the knapsack
...
ni means the number of type i packed into the knapsack

Total weight of packed item=w1×n1+w2×n2+...+wi×ni.
Total value of packed item=v1×n1+v2×n2+...+vi×ni.
So the problem is: max the v1×n1+v2×n2+...+vi×ni such that w1×n1+w2×n2+...+wi×ni<=W.
```
#### Top-down recursion
We can divide the problem into smaller problems when using recursion to solving this kind of problem. Suppose we choose `first item` 1 with `w1` into  the bag, we can delegate the subproblems which is to max the total value with `W-w1` for others to solve. Other items can be considered as the similar situation. Based on the subprolems, I can solve the original problem with as follows: 
```
subsol1=maximum total value with `W-w1`. 
final1=the first item packed was item 1
final1=subsol1+v1
...

Max value=max(final1,final2,final2...finali)
```
Based on the above, the `divide and conquer` solution for the unbounded knapsack problem:
```python
def unbound_knapSack_top(wts,vals,maxw):
    n=len(vals)
    subsol=[0 for _ in xrange(n)]
    finsol=[0 for _ in xrange(n)]

    # base case
    if maxw==0:
        return 0

    # smaller problem using recursion
    for i in xrange(n):
        if wts[i]<=maxw:
            # capacity reduce by wts[i] since it already has item i
            subsol[i]=unbound_knapSack_top(wts,vals,maxw-wts[i])

    # based on the above solution, solve the original problem
    for i in xrange(n):
        if wts[i]<=maxw:
            # capacity reduce by wts[i] since it already has item i
            finsol[i]=subsol[i]+vals[i]

    # find the maximum
    return max(finsol)
```
Also, we can solve this problem using `dynamic problem`. It's much similiar with recusive solution. Every time the `maxw` has change by the `parameter` to call the recusive method. 

#### Bottom-up memorization
```python
def unbound_knapSack_bottom(wts,vals,maxw):
    n=len(vals)
    subsol=[0 for _ in xrange(n)]
    finsol=[0 for _ in xrange(n)]
    # to save the recursive result
    dp=[0 for _ in xrange(maxw+1)]

    # base case
    if maxw==0:
        return 0

    for submaxw in xrange(1,maxw+1):
        # inner part is the recusive code
        for i in xrange(n):
            if wts[i]<=submaxw:
                # capacity reduce by wts[i] since it already has item i
                subsol[i]=dp[submaxw-wts[i]]

        # based on the above solution, solve the original problem
        for i in xrange(n):
            if wts[i]<=submaxw:
                # capacity reduce by wts[i] since it already has item i
                finsol[i]=subsol[i]+vals[i]

        # find the maximum for the submaxw
        dp[submaxw]=max(finsol)
    return dp[maxw]
```
Based on the above main loops, it's easy to see that the total time complexity is `O(n*maxw)`.

### Bounded Knapsack{#bkp}
For the bounded Knapsack, it's easy to convert it to the 1/0 knapsack. For example, values with `[2,3,4]` and the corresponding counts are `[1,2,2]`, we can convert the values array to `[2,3,3,4,4]`. Therefore, it becomes the 1/0 knapsack problem. 
```python
def bounded_knapSack_dp(wts,vals,cnts,maxw):
    n=len(vals)
    if n==0 or maxw==0 :
        return 0
    dp=[[0 for _ in xrange(maxw+1)] for _ in xrange(n+1)]

    for i in xrange(1,n+1):
        for j in xrange(maxw+1):
            if wts[i-1]<=j:
                # get the min total count subject to weight j
                for k in xrange(1,min(cnt[i-1],j/wts[i-1])+1):
                    dp[i][j]=max(dp[i-1][j],dp[i-1][j-k*wts[i-1]]+k*vals[i-1])
            else:
                dp[i][j]=dp[i-1][j]
    return dp[n][maxw]
```
>>Think a little more, how to improve the above data structure to know what's in the optimized knapsack not just the total values?

### Group Knapsack{#gkp}
The group knapsack also can be solved using the `1/0 knapsack`. We need to find the previous max value in the weight item list or just one item. The code might be much similar with the `1/0 knapsack`.
```python
def group_knapSack(wts,vals,maxw):
    n=len(vals)
    if n == 0 or maxw==0:
        return 0
    dp=[[0 for _ in xrange(maxw+1)] for _ in xrange(n+1)]

    for i in xrange(1,n+1):
        for j in xrange(len(wts[i-1])):
            for w in xrange(maxw+1):
                if wts[i-1][j]<=w:
                    dp[i][w]=max(dp[i][w],dp[i-1][w],dp[i-1][w-wts[i-1][j]]+vals[i-1][j])
                else:
                    # check the max update value
                    dp[i][w]=max(dp[i][w],dp[i-1][w])
    return dp[n][maxw]
```
The dp table will update `wts[i-1]` times, so you should be careful about the `w` range. To save the space, we can improve the space complexity since the relationship happens bettween the previous array and current array.
```python
def group_knapSack_space(wts,vals,maxw):
    n=len(vals)
    if n == 0 or maxw==0:
        return 0
    pre=[0 for _ in xrange(maxw+1)]
    cur=[0 for _ in xrange(maxw+1)]

    # deal with item 1-n
    for i in xrange(1,n+1):
        cur=[0 for _ in xrange(maxw+1)]
        for j in xrange(len(wts[i-1])):
            for w in xrange(maxw+1):
                if wts[i-1][j]<=w:
                    cur[w]=max(cur[w],pre[w],pre[w-wts[i-1][j]]+vals[i-1][j])
                else:
                    cur[w]=max(cur[w,pre[w])
        # swap
        pre,cur=cur,pre
    return pre[-1]
```

### Q&A{#qa}
Given *N* stacks, each stack contains *Si* elements. A array has a capacity with* M*. Find numbers in the *N* stacks to put them into the array such that the sum of the number in the array is the maximum. To choose the number in the stacks, it only support to get the top number. For example, `S=[1,200,1,2,3]`, if you want to get the number 200, you need also choose `3,2,1` as the candicate numbers if the array has space to store those values.
```
Example:
S1=[1,1,100,3] S2=[2000,2,3,1]  S3=[10,1,4] 
the maximum sum of the 3 numbers is 3+100+4=107. 
```
>> I strongly recommend you to try this yourself first.

&nbsp;

&nbsp;

&nbsp;

&nbsp;

To solve this problem, we can transfer it into group Knapsack as follows:

|        | G1              | G2           | G3       |
|--------|-----------------|--------------|----------|
| Weight | [1,2,3,4]       | [1,2,3,4]    | [1,2,3]  |
| Values | [3,103,104,105] | [1,4,6,2006] | [4,5,15] |
The capacity is 3 number. The answer is to choose G1 with weight 3 and G3 with weight 1.The maxinum is 107. The code can be like this:
```python
def max_stack_sum(stacks,m):
    if not stacks:
        return 0
    vals,wts=[],[]
    for st in stacks:
        for i in xrange(len(st)-2,-1,-1):
            st[i]+=st[i+1]
        vals.append(st[::-1])
        wts.append(range(1,len(st)+1))
    return group_knapSack_space(wts,vals,m)
```

## Reference{#ref} 
* Dynamic Programming for 0/1 [Knapsack](http://www.geeksforgeeks.org/dynamic-programming-set-10-0-1-knapsack-problem/).
* [Unbounded Knapsack](http://www.mathcs.emory.edu/~cheung/Courses/323/Syllabus/DynProg/knapsack2.html) at mathcs.
* [Bounded knapsack](https://www.codeproject.com/articles/706838/bounded-knapsack-algorithm) at code project.
* [More knapsack](http://love-oriented.com/pack/#sec4).
